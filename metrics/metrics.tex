\documentclass{article}
\usepackage{graphicx} % Images
\usepackage{amsmath,amsthm} % Math
\usepackage{wasysym} % Math symbols
\usepackage{amssymb} % Greek alphabets
\usepackage{mathrsfs,amsfonts,calrsfs} % Math fonts
\usepackage{newtxtext}
\usepackage{geometry} % Formatting
\usepackage[dvipsnames,svgnames]{xcolor}
\usepackage[strict]{changepage}
\usepackage{framed}
\usepackage{cancel}
\usepackage{autobreak}
\usepackage{hyperref}
\usepackage{indentfirst} % 让章节后第一段也缩进
%graphicspath{ {./images/} }


\setlength{\parindent}{2em} % 控制首行缩进
\setlength{\parskip}{0.3\baselineskip} % 控制段落间距

\providecommand{\tightlist}{
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}}

\newcommand*\sepline{%
  \begin{center}
    \rule[1ex]{.5\textwidth}{.5pt}
  \end{center}}





% Blue block
\definecolor{blueshade}{rgb}{0.95,0.95,1} % blue block color
\newenvironment{blueblock}{
\def\FrameCommand{
  \hspace{1pt}
    {\color{DarkBlue}
    \vrule width 2pt}
    {\color{blueshade}
    \vrule width 4pt}
  \colorbox{blueshade}
}
\MakeFramed{
  \advance
  \hsize-
  \width
  \FrameRestore}
\noindent\hspace{-4.55pt}% disable indenting first paragraph
\begin{adjustwidth}{}{7pt}
\vspace{2pt}\vspace{2pt}
}
{\vspace{2pt}\end{adjustwidth}\endMakeFramed}


% Green block
\definecolor{greenshade}{rgb}{0.90,0.99,0.91} % green block
\newenvironment{greenblock}{%
\def\FrameCommand{%
  \hspace{1pt}%
    {\color{Green}%
    \vrule width 2pt}%
    {\color{greenshade}%
    \vrule width 4pt}%
  \colorbox{greenshade}%
}%
\MakeFramed{%
  \advance%
  \hsize-%
  \width%
  \FrameRestore}%
\noindent\hspace{-4.55pt}% disable indenting first paragraph
\begin{adjustwidth}{}{7pt}%
\vspace{2pt}\vspace{2pt}%
}
{%
\vspace{2pt}\end{adjustwidth}\endMakeFramed%
}


% Red block
\definecolor{redshade}{rgb}{1.00,0.90,0.90} % red block
\newenvironment{redblock}{
\def\FrameCommand{
  \hspace{1pt}
    {\color{LightCoral}
    \vrule width 2pt}
    {\color{redshade}
    \vrule width 4pt}
  \colorbox{redshade}
}
\MakeFramed{
  \advance
  \hsize-
  \width
  \FrameRestore}
\noindent\hspace{-4.55pt}% disable indenting first paragraph
\begin{adjustwidth}{}{7pt}
\vspace{2pt}\vspace{2pt}
}
{\vspace{2pt}\end{adjustwidth}\endMakeFramed}


\newtheorem{question}{Question}
\newtheorem{note}{Note}

\title{Notes on Econometrics}
\author{Victor Li}
\date{Autumn Sememster, 2023}

\begin{document}

\maketitle

\newpage
\tableofcontents


\newpage
\section{Preparation and foundational knowledge}

Reading this note book, you should understand all the basics of higher math. 

\subsection{Core of econometrics} % (fold)
\label{sub:core_of_econometrics}

A student studying econometrics should be able to differentiate that
\begin{itemize}
\tightlist
  \item causal relationship
  \item correlative relationship
\end{itemize}
are two different things. The latter may be intuitive in live but is essentially misguiding in true meaning.
\footnote{Modern day science has brought us intuitive idea. The simple and commonly accepted idea of science is that agnosticism and determinism are required if you want to be rational and truth-seeking. Which, is mostly true.}

\textbf{The core pursuit of econometrics is to move beyond simple observation to rigorously estimate causal relationships.} 

We often observe that two things move together—a correlation—but the goal is to determine if a change in one causes a change in the other. For example, an online retailer might see that on days with high advertising spending, they also have high sales. This is a correlation. But did the advertising cause the increase in sales, or did both rise because of an external factor, like a holiday weekend? Econometrics provides the theoretical framework and practical tools to answer such questions with data.

Only with this kind of understanding, you can bear in mind that the core mission of econometrics is to use statistical methods and mathematical models to give empirical content to economic theory. In simpler terms, it's about \textbf{turning broad economic ideas into testable, quantifiable statements}. This is what sets econometrics apart from other causal studies.






\subsection{Data structure} % (fold)
\label{sub:data_structure}

% subsection data_structure (end)

The data we use dictates the methods we can apply. Econometric data is typically organized in one of three ways:

\begin{itemize}
  \item \textbf{Cross-Sectional Data}: A snapshot of many different entities at a single point in time.
  Example: Data on daily sales and advertising expenditure for 500 different online stores on December 1st, 2023. Each row is a different store.

  \item \textbf{Time Series Data}: Observations of a single entity over multiple time periods.
  Example: Data on the daily sales and advertising expenditure for one specific online store from January 1st to December 31st, 2023. Each row is a different day.

  \item \textbf{Panel Data (Longitudinal Data)}: A combination of the two, observing multiple entities over multiple time periods.
  Example: Data on the daily sales and advertising expenditure for 500 different online stores, tracked each day for the entire year of 2023. This is incredibly powerful as it allows us to control for factors unique to each store that don't change over time.
\end{itemize}



\subsection{Understanding the Shape of Your Data: Moments} % (fold)
\label{sub:understanding_the_shape_of_your_data_moments}

% subsection understanding_the_shape_of_your_data_moments (end)



Before modeling, we must understand the fundamental characteristics of our variables. The shape of a variable's distribution can be summarized by its statistical moments.

\paragraph{Skewness}

Skewness measures the asymmetry of a distribution. A perfectly symmetric distribution has zero skewness.

\begin{greenblock}
\textbf{Definition: Skewness} (Standardized 3rd Central Moment)
\begin{equation}
\tilde \mu_3=\frac{E[(Y-\bar Y)^3]}{\sigma^3_Y}
\end{equation}
The skewness of a random variable $Y$ is the average of its cubed standardized deviations. Cubing the deviations preserves their sign.
\end{greenblock}


\begin{itemize}
  \item $\approx 0 \iff $ Symmetric, almost like normal distribution.

  \item $>0 \iff $ Right Skew, right side lower, meaning more outliers ar right side.
  
  \item $<0 \iff $ Left Skew, left side lower, meaning more outliers ar left side.
\end{itemize}
  

\paragraph{Kurtosis}  


Kurtosis measures the "tailedness" of a distribution. It tells us how much of the data's variance is driven by infrequent, extreme events (fat tails) versus frequent, modest deviations.
\begin{greenblock}
\textbf{Definition: Kurtosis} (Standardized 4th Central Moment)
\begin{equation}
Kurt=\frac{E[(Y-\bar Y)^4]}{\sigma^4_Y}
\end{equation}
The kurtosis of $Y$ is the average of its standardized deviations raised to the fourth power. The fourth power makes extreme values dominate the calculation.
\end{greenblock}

For a normal distribution, the kurtosis is 3. Based on this standard,
\begin{itemize}
  \item $\approx 3 \iff$ (Mesokurtic): The distribution has tails similar to a normal distribution.
  \item $>3 \iff$ (Leptokurtic): "Fat tails." The distribution has more mass in its tails than a normal distribution. In finance, this implies that extreme market movements (crashes or booms) are more likely than a normal model would predict.
  
  \item $<3 \iff$ (Platykurtic): "Thin tails." Extreme events are less likely than in a normal distribution.
\end{itemize}


\section{Linear Regression Model}

The linear regression model is the workhorse of econometrics. It provides a simple yet powerful way to model how a dependent variable, $Y$ changes in response to an independent (or explanatory) variable, $X$.

\subsection{The Population and the Sample}

It is crucial to distinguish between the unobservable reality we wish to understand and the limited data we have to work with. The reality is the population of data, the part of reality that we are able to direct observe is the sample of the population.


\paragraph{The Population Regression Function (PRF)}
This is the true, underlying relationship that governs how $Y$is determined. It is a theoretical ideal that we can never observe directly.


For a simplified version of function format (that is the simple form of linear function), the PRF can be stated as:
\begin{align}
& E(Y|X)=\beta_{0}+\beta_{1} X =E(Y|X) \text{ (regression equation)}
\\& Y= \beta_{0}+\beta_{1}X+\underbrace{\mu}_\text{disturbance}=E(Y|X)+\mu \text{ (regression model)}
\end{align}

\begin{itemize}
  \item $\beta_0$ (Intercept) and $\beta_1$(Slope) are the population parameters. They are fixed, unknown constants. $\beta_1$ is typically the object of our interest; it represents the true causal effect on $Y$ of a one-unit change in $X$.
  \item $\mu$ is the unobservable disturbance or error term. It captures all other factors that affect $Y$apart from $X$, as well as any inherent randomness. In our retail example, if $Y$is sales and $X$is ad spend, $\mu$includes competitor actions, news events, website glitches, and customer mood.
\end{itemize}







\paragraph{The Sample Regression Function (SRF)}

Since we cannot see the entire population, we use a random sample of data to estimate the PRF. The SRF is the estimated relationship for our specific sample.

\begin{align}
  &\hat Y_i =\hat \beta_0 +\hat{\beta_{1}}X_i \text{ (regression equation)}
  \\& Y_i=\hat \beta_0 +\hat{\beta_{1}}X_i+
  \underbrace{e}_\text{error}
  =\hat Y_i + e_i
  \text{ (regression model)}
\end{align}

\begin{itemize}
  \item $\hat{\beta_0}$ and $\hat{\beta_1}$ are estimators (or coefficients). They are our data-driven "best guesses" for the true population parameters $\beta_0$ and $\beta_1$. The "hat" notation ($\hat{\cdot}$) always denotes an estimate.
  \item $e_i$ is the residual. It is the sample counterpart of the disturbance $\mu$ and represents the difference between the actual value $Y_i$ and the predicted value from our model, $\hat{Y_i} = \hat{\beta_0} +\hat{\beta_{1}}X_i$. Thus, $e_i = Y_i - \hat{Y_i}$.
\end{itemize}



\subsection{OLS Estimator} % (fold)
\label{sub:ols_estimator}


\paragraph{The Ordinary Least Squares (OLS) Estimator}

How do we choose the best estimates $\hat{\beta_0}$ and $\hat{\beta_1}$ to draw a line through our data points? The OLS method provides the answer: we choose the values that minimize the sum of the squared residuals.

The goal is to minimize the deviation of estimation from the real world
\begin{align}
  & \min \sum\limits_{i=1}^n (Y_i-\hat Y_i)^2
  \\& =\min \sum\limits_{i=1}^n e_i^2
  \\& =\min \sum\limits_{i=1}^n [Y_i-(\hat{\beta_{0}}+\hat{\beta_{1}}X_i)]^2
\end{align}

\begin{greenblock}
\textbf{The OLS Principle}
The goal is to find the line that is, on the whole, "closest" to all the data points. We define "closeness" as the vertical distance ($e_i$). By squaring each residual, we ensure that negative and positive deviations don't cancel out and that larger errors are penalized more heavily.
\begin{align}
  \min_{\hat{\beta_0}, \hat{\beta_1}} \sum\limits_{i=1}^n e_i^2 = \min_{\hat{\beta_0}, \hat{\beta_1}} \sum\limits_{i=1}^n [Y_i-(\hat{\beta_{0}}+\hat{\beta_{1}}X_i)]^2
\end{align}
\end{greenblock}

So OLS is basically an optimization problem.

FOC:
$\begin{cases}
\frac{\partial \min}{\partial \hat{\beta_{0}}}=0
\\
\frac{\partial \min}{\partial \hat{\beta_{1}}}=0
\end{cases} \Rightarrow
\text{yielding the optimal coefficients}
\begin{cases}
\hat{\beta_{0}}=\bar Y-\hat{\beta_{1}}\bar X
\\
\hat{\beta_{1}}=\frac{S_{XY}}{S^2_X}=\frac{\sum\limits_{i=1}^n (X_i-\bar X)(Y_i-\bar Y)}{\sum\limits_{i=1}^n (X_i-\bar X)^2}
\end{cases}$


Using OLS, we would have fitted value $\hat Y_i$ and residual value $\hat e_i$
\begin{equation}
  \begin{cases}
    \hat Y_i=\hat{\beta_{0}}+\hat{\beta_{1}}X_i,i=1,2\dots n
    \\
    \hat e_i=Y_i-\hat Y_i,i=1,2\dots n
  \end{cases}
\end{equation}




\subsection{The Gauss-Markov Theorem and BLUE}

Why should we prefer the OLS method over any other way of fitting a line? The Gauss-Markov theorem provides the theoretical justification. It states that if a set of assumptions holds, then the OLS estimator is the Best Linear Unbiased Estimator (BLUE).

The Gauss-Markov Assumptions (Classical Linear Regression Model - CLRM) is required by:
\begin{enumerate}
  \item Linearity in Parameters: The model is linear in $\beta_0$ and $\beta_1$.
  \item Random Sampling: The data is a random sample from the population.
  \item Variation in X: The sample outcomes for $X$ are not all the same value.
  \item Zero Conditional Mean ($E(\mu|X) = 0$): This is the most critical assumption. It states that the unobserved factors in $\mu$ are, on average, unrelated to the value of $X$. In our example, it means that a competitor's promotion (part of $\mu$) is not systematically launched on days when we happen to increase our ad spend ($X$). A violation of this assumption leads to biased estimates.
  \item Homoskedasticity ($var(\mu|X) = \sigma^2$): The variance of the unobserved factors is constant for all values of $X$. This means the "unpredictability" of sales is the same on high-spend ad days as it is on low-spend ad days.
\end{enumerate}


\begin{blueblock}
\begin{note}[What is BLUE?]
If the five Gauss-Markov assumptions hold, the OLS estimator has the following desirable properties:
\end{note}
\begin{itemize}
    \tightlist
    \item \textbf{Best:} It has the minimum variance among all linear unbiased estimators. This means OLS is the most precise or efficient.
    \item \textbf{Linear:} The estimators $\hat{\beta_0}$ and $\hat{\beta_1}$ are linear functions of the dependent variable $Y$.
    \item \textbf{Unbiased:} On average, the estimator will equal the true population parameter. Formally, $E(\hat{\beta})=\beta$. Your estimate from one sample may be high or low, but if you could repeat the sampling process infinitely, the average of your estimates would be the true value.
    \item \textbf{Estimator:} It is a rule that tells us how to use data to compute an estimate of a population parameter.
\end{itemize}
In essence, the theorem gives us confidence that, under ideal conditions, OLS is the optimal choice. Much of advanced econometrics is concerned with what to do when one or more of these assumptions are violated.
\end{blueblock}



\subsection{Measures of Fit}

Once we have estimated a regression model using OLS, a natural question arises: how well does our model actually fit the data? We need metrics to quantify the model's explanatory power.

\paragraph{Decomposing Variance}

The foundation of the most common goodness-of-fit measure is the decomposition of the total variation in the dependent variable, $Y$. The total variation is the sum of the squared deviations of each $Y_i$ from its mean $\bar Y$. This is called the \textbf{Total Sum of Squares (TSS)}.

This total variation can be broken into two parts: the portion that is explained by our model, called the \textbf{Explained Sum of Squares (ESS)}, and the portion that is left unexplained, which is captured by the residuals and is called the \textbf{Sum of Squared Residuals (SSR)}.

\begin{equation}
    \underbrace{\sum_{i=1}^n (Y_i - \bar Y)^2}_{\text{TSS}} = \underbrace{\sum_{i=1}^n (\hat Y_i - \bar Y)^2}_{\text{ESS}} + \underbrace{\sum_{i=1}^n e_i^2}_{\text{SSR}}
\end{equation}


\paragraph{Degree of Freedom} 

In statistics, degrees of freedom (df) refers to the number of values in a final calculation that are free to vary. A good way to think about it is as the number of independent pieces of information that you can use to estimate a parameter.

\begin{blueblock}
\begin{note}[Degree of freedom]
\end{note}
For the decomposition, degree of freedom is actually $\begin{cases}
 TSS: n-1\\
 ESS: k\\
 SSR: n-k-1
\end{cases}$.

Denominators below are actually degree of freedom. Only in large samples approximated by $n$.
\end{blueblock}

For $Y$,
\begin{align}
&\frac{TSS}{n}=var(Y)
\\& TSS=n \cdot var(Y)=n \cdot \frac{\sum\limits_i^n (Y_i-\bar Y)^2}{n}=\sum\limits_i^n (Y_i-\bar Y)^2
\\& SE(Y)=\sqrt{var(Y)}=\sqrt{\frac{TSS}{n}}=\sqrt{\frac{\sum\limits_i^n (Y_i-\bar Y)^2}{n}}
\end{align}

For $\hat Y$,
\begin{align}
&\frac{ESS}{n}=var(\hat Y)
\\&ESS=\sum\limits_i^n (\hat Y_i-\bar Y)^2
\\&SE(\hat Y)=\sqrt{var(\hat Y)}=\sqrt{\frac{ESS}{n}}=\sqrt{\frac{\sum\limits_i^n (\hat Y_i-\bar Y)^2}{n}}
\end{align}

For $e$,
\begin{align}
&\frac{SSR}{n}=var(e)
\\&SSR=\sum\limits_i^n (e_i-\bar e)^2
\\&SER=SE(e)=\sqrt{var(e)}=\sqrt{\frac{SSR}{n}}=\sqrt{\frac{\sum\limits_i^n (e_i-\bar e)^2}{n}} \text{ (also the SE of the regression)}
\end{align}

\paragraph{R-squared}

A common and easy way to measure goodness of fit is by using $R^2$. It is considered an indicator to judge a model.

The $R^2$, or the coefficient of determination, formalizes this decomposition into a single, intuitive metric. It measures the fraction of the total variance in $Y$ that is explained by the explanatory variable(s) in the model.

\begin{align}
& R^2=\frac{ESS}{TSS}=\frac{\sum\limits_i^n (\hat Y_i-\bar Y)^2}{\sum\limits_i^n (Y_i-\bar Y)^2}
\\&R^2=\frac{ESS}{TSS}=1-\frac{SSR}{TSS}=\frac{\text{explained by the estimated model}}{\text{total sample data}}
\end{align}



\paragraph{Standard Error of the Regression (SER)}
While $R^2$ is a relative measure of fit, the SER is an absolute measure. It estimates the standard deviation of the regression disturbance $\mu$. In practical terms, it tells us the typical size of the regression error, or how far our predictions typically are from the actual outcomes.

\begin{greenblock}
\textbf{Definition: Standard Error of the Regression (SER)}
\begin{equation}
SER = s_e = \sqrt{\frac{SSR}{n-2}} = \sqrt{\frac{\sum\limits_i^n e_i^2}{n-2}}
\end{equation}
The SER is measured in the same units as the dependent variable, $Y$.
\end{greenblock}

A lower SER implies a more accurate model in terms of prediction. In our retail example, if sales ($Y$) are measured in dollars, an SER of \$500 means our model's predictions of daily sales are typically off by about \$500.


% subsection r_2 (end)


\section{Hypothesis and Test}

Remember the metrics before are used to test how good a model is. When a number is calculated, is it 100 percent convincing? No, they are not. Because they are calculated under assumptions and simplization of reality. 

Would a different sample produce a different estimate? The fundamental question of statistical inference is: how confident are we that our estimated relationship is real and not just a fluke of our particular sample? For instance, is the true effect of advertising on sales, $\beta_1$, actually zero? 

\textbf{We care about significance in statistics because it provides a way to quantify the likelihood that an observed result in a study is not due to random chance.}

Hypothesis testing provides a formal framework to answer this.

\subsection{The t-test} 

The most common method for testing a hypothesis about a {\color{red} single} regression coefficient is the t-test.\footnote{Take a good look at the word "single" and remember what it stands.} It follows a structured process to determine whether to accept or reject a claim about the true population parameter.

\textbf{Step 1: State the Hypotheses}

We begin by stating a \textbf{null hypothesis ($H_0$)}, which represents the "status quo" or a benchmark of no effect, and an \textbf{alternative hypothesis ($H_1$)}, which is what we are trying to establish. The most common test is for statistical significance:

$\begin{cases}
  H_0: \beta_1  = 45812
  \\
  H_1: \beta_1 \neq 45812
\end{cases}$

This is a two-sided test, as we are interested in deviations from zero in either direction (positive or negative).

\textbf{Step 2: Calculate the t-statistic}

The t-statistic (or t-value) measures how many standard errors our estimated coefficient, $\hat{\beta_1}$, is away from the value hypothesized under the null. A larger t-statistic implies that our estimate is less likely to have occurred by random chance if the null hypothesis were true.

\begin{greenblock}
\textbf{Definition: t-statistic}
\begin{equation}
  t=\frac{\hat{\beta_{1}}-\beta_{1, H_0}}{se(\hat{\beta_{1}})}=\frac{\text{Estimation}-\text{Hypothesized Value}}{\text{Standard Error of Estimation}}
\end{equation}
where $se(\hat{\beta_1})$ is the standard error of our coefficient estimate, a measure of its sampling variability. When testing for significance, $\beta_{1, H_0}$ is 0.
\end{greenblock}

\textbf{Step 3:  Make a Decision}

We have two common, and equivalent, ways to decide whether our t-statistic is "large enough" to reject the null hypothesis.


\textbf{The p-value Approach:} The p-value is the probability of observing a t-statistic as extreme as, or more extreme than, the one we calculated, assuming the null hypothesis is true.
\begin{equation}
  p=2 \Phi(-|t|)
\end{equation}
Here, $\Phi$ is the cumulative distribution function of the standard normal distribution (a good approximation for the t-distribution in large samples). We compare the p-value to a pre-determined \textbf{significance level ($\alpha$)}, usually 0.05 (5\%), 0.01 (1\%), or 0.10 (10\%).

\begin{itemize}
\tightlist
  \item If $p < \alpha$, we \textbf{reject the null hypothesis}. The result is "statistically significant at the $\alpha$ level." We have strong evidence that $\beta_1$ is not zero.
  \item If $p \ge \alpha$, we \textbf{fail to reject the null hypothesis}. The result is "not statistically significant." We do not have sufficient evidence to claim that $\beta_1$ is different from zero.
\end{itemize}

\begin{blueblock}
$p=2 \Phi(-|t|) 
\begin{cases}
  < \alpha \iff \text{at reject area} \iff \text{reject null hypothesis $H_0$} \iff \text{$X$ is significant}
  \\
  > \alpha \iff \text{at accept area} \iff \text{accept null hypothesis $H_0$} \iff \text{$X$ is not significant}
\end{cases}$
\end{blueblock}



\textbf{The Critical Value Approach:} Alternatively, we can find a critical value, $t_c$, from a t-distribution table (or software) that corresponds to our chosen significance level $\alpha$ and degrees of freedom ($df=n-2$).

\begin{itemize}
\tightlist
  \item If $|t| > t_c$, our calculated statistic falls in the "rejection region." We \textbf{reject the null hypothesis}.
  \item If $|t| \le t_c$, our statistic falls in the "acceptance region." We \textbf{fail to reject the null hypothesis}.
\end{itemize}


\begin{blueblock}
$|t|
\begin{cases}
>t_{\frac{\alpha}{2}} \iff \text{at reject area} \iff \text{reject null hypothesis $H_0$} \iff \text{$X$ is significant}
\\
<t_{\frac{\alpha}{2}} \iff \text{at accept area} \iff \text{accept null hypothesis $H_0$} \iff \text{$X$ is not significant}
\end{cases}$
\end{blueblock}


% subsection t_test (end)

\subsection{Confidence Interval} % (fold)
\label{subp:confidence_interval}

While a t-test gives a yes/no answer about a single hypothesized value, a confidence interval provides a more informative range of plausible values for the true population parameter, $\beta_1$.

\paragraph{Constructing a Confidence Interval}
A 95\% confidence interval is constructed by taking our point estimate and adding and subtracting a margin of error, which is determined by the critical t-value and the standard error of the estimate.

\begin{greenblock}
\textbf{Formula: (1-$\alpha$)\% Confidence Interval}
\begin{equation}
  CI = [\hat{\beta_1} - t_{c} \cdot se(\hat{\beta_1}), \quad \hat{\beta_1} + t_{c} \cdot se(\hat{\beta_1})]
\end{equation}
For a 95\% confidence interval, $\alpha=0.05$, and $t_c$ is the critical value leaving $\alpha/2 = 2.5\%$ in each tail of the t-distribution. For large samples, $t_c \approx 1.96$.
\end{greenblock}

A confidence interval can also be used for hypothesis testing. To test the null hypothesis $H_0: \beta_1=0$, we simply check if 0 lies within the interval.
\begin{itemize}
\tightlist
    \item If the interval \textbf{does not} contain 0, we can reject $H_0$ at the corresponding significance level.
    \item If the interval \textbf{does} contain 0, we fail to reject $H_0$.
\end{itemize}
This provides a measure of both statistical significance and the practical range of uncertainty around our estimate. A very wide interval, even if it excludes zero, signals that our estimate is imprecise.

\begin{blueblock}
\begin{note}[one-tale or two-tale?]
depending on the hypothesis
\end{note}
\end{blueblock}

% subparagraph confidence_interval_test (end)


\section{multi-variate linear regression} % (fold)
\label{sec:multi_variate_linear_regression}

\textbf{New assumption for MLR:}

non-zero finite fourth order moment (kurtois)


\textbf{OVB, Omitted Variable Bias }
$\Rightarrow 
\begin{cases}
  E(\mu|X)\neq 0 \text{ endogeneity}  
  \\
  R^2 \text{ is lower than it should be} 
\end{cases}$

\begin{equation}
  \hat{\beta}\stackrel{P}{\rightarrow}\beta+
  \color{red}
  \underbrace{\frac{\sigma_u}{\sigma_X}\rho_{uX}}_{\text{effect of OVB}}
\end{equation}

meaning OVB causes estimator to be biased and inconsistent

\textbf{How to overcome OVB?}
\begin{itemize}
\tightlist
  \item More control variables
  \item IV
  \item Panel Fixed Effect model
\end{itemize}

\sepline

\textbf{Adjusted R-squared}
\begin{equation}
  \bar R^2=1-\frac{RSS/n-k-1}{TSS/n-1}=1-\frac{n-1}{n-k-1}\frac{RSS}{TSS}
\end{equation}

\begin{blueblock}
\begin{note}[difference between $R^2$ and $\bar R^2$]
\end{note}
$\bar R^2<R^2$

$R^2 \in (0,1)$ whereas $\bar R^2 $ can be sub zero.
\end{blueblock}


\textbf{how many variables should i add into the model?}

AIC

BIC

\sepline

\textbf{OLS in MLR:}

\begin{equation}
  \min\limits_{\{\beta_0,\dots,\beta_k\}} \sum\limits_i^n (Y_i-\hat Y_i)^2
\end{equation}

or in matrix form
\begin{equation}
  \min (Y-X \hat{\beta} )^2
\end{equation}

results
\begin{equation}
  \hat{\beta}=(X^T X)^{-1}X^T Y
\end{equation}

\sepline

\textbf{Joint hypothesis test}
\begin{equation}
  \begin{cases}
    H_0: \beta_1=0 \& \beta_2=0
    \\
    H_1: \beta_1\neq 0 \text{ or } \beta_2\neq 0\text{ or both} \neq 0 
  \end{cases}
\end{equation}


\textbf{F-test}
\begin{equation}
  F=\frac{1}{2}(\frac{t_1^2+t_2^2-2 \hat \rho_{t_1,t_2} t_1 t_2 }{1-\hat \rho_{t_1,t_2}^2}) \text{, where $\hat\rho_{t_1,t_2}^2$ is estimated correlative coefficient}
\end{equation}

in large sample $\hat\rho_{t_1,t_2}^2 \stackrel{P}{\rightarrow} 0$, therefore

\begin{equation}
  F=\frac{1}{2}(t_1^2+t_2^2)
\end{equation}

\textbf{simplified F statistics when homoskedasticity}

$H_0: \beta_1=0,\beta_2=0,\dots,\beta_q=0$ and $H_1: \dots$

$q=$ number of constraints

unrestricted regression: $Y=Y(X_1,X_2\dots X_n)$

restricted regression: $Y=Y(X_1,X_2\dots X_i) \;s.t. \; g(X_i,X_{i+1}\dots X_n)=c$

\begin{equation}
  F=
  \frac{(R^2_{\text{unrestricted}}-R^2_{\text{restricted}})/q}{(1-R^2_{\text{unrestricted}})/(n-k_{\text{unrestricted}}-1)}
\end{equation}


\textbf{Tests for Single Constraints Involving Multiple Coefficients}

change the original
$Y=\beta_{0}+\beta_{1}X+\beta_{2}Y+u$ 

to
\begin{equation}
\begin{split}
  Y=&\beta_{0}+(\beta_1- \beta_{2})X+\beta_2 (X+Y)+u\\
  =&\beta_{0}+\gamma X +\beta_2 W+u
\end{split}
\end{equation}

now testing $\gamma=0$ is same as testing $\beta_{1}=\beta_{2}$

% section multi_variate_linear_regression (end)



\section{other regression stuff}

\textbf{dummy variables}

$D_i=0 \text{ or } 1$

\textbf{dummy variable trap}

For 4 cases, model has 4 cases $\Rightarrow$ perfect multicolineary

To fix it, use $k-1$ dummies for $k$ cases.

\sepline

non-linear regression

probit model, $Pr(Y=1|X_1,X_2\dots X_n)=\Phi(\beta_0 +\beta_{1}X_1+\dots+\beta_{n} X_n)$

logit model, $Pr(Y=1|X_1,X_2\dots X_n)=\frac{1}{1+e^{-(\beta_0 +\beta_{1}X_1+\dots+\beta_{n} X_n)}}$


\sepline

\textbf{heteroskedasticity}

$var(u|x)$ is variant to different $x_i, i\in n$

heteroskedasticity causes significance test to be meaningless

how to overcome

\begin{itemize}
\tightlist
  \item heteroskedasticity-robust standard error regression
  \item GLS
  \item clustered heteroskedasticity-robust standard error regression
\end{itemize}

\sepline

multicolinearity

\sepline

interaction term

1) two dummies (DID)

$Y=\beta_0+\beta_{1}D_1+\beta_{2}D_2+\beta_{3}D_1 D_2+u$

\begin{equation}
  \begin{split}
  \text{effect of $D_2$}=&E(Y|D_1,D_2=1)-E(Y|D_1,D_2=0)\\
  =&(\beta_0+\beta_{1}D_1+\beta_{2}+\beta_{3}D_1)-(\beta_0+\beta_{1}D_1)
  \\=&\beta_2+\beta_3 D_1
  \end{split}
\end{equation}

2) dummy and continuent variable


3) two continuent variables



\section{Panel data}

\textbf{fixed effect}

fixed effect is used when $corr(X,u)\neq 0$. we use dummies on individual level to capture fixed effect, eliminating endogeneity.

individual fixed effect
\begin{equation}
  y_{it}=\beta_1 X_{it}+\beta_2D_i+\mu_{it}
\end{equation}

time fixed effect
\begin{equation}
  y_{it}=\beta_1 X_{it}+\beta_2D_t+\mu_{it}
\end{equation}

individual and time fixed effect
\begin{equation}
  y_{it}=\beta_1 X_{it}+\beta_2D_i+\beta_3D_t+\mu_{it}
\end{equation}


\begin{blueblock}
\begin{note}[individual fixed effect]
can be used to overcome OVB problem
\end{note}
\end{blueblock}



\textbf{random effect}

random effect is used when $corr(X,u)=0$

\textbf{Hausman test}

used to decide whether to use fixed effect or randome effect

The null hypothesis is that there is no difference between random effects and fixed effects. If the null hypothesis is rejected, the fixed effects model is adopted, otherwise the random effects model is adopted.

\sepline

Long panel, $n<t$

Short panel, $n>t$

For short panels, since $t$ is small, it is impossible to explore whether the disturbance term has autocorrelation. For long panels, $t$ is relatively large, so it is necessary to discuss its heteroskedasticity and autocorrelation.

GMM


\section{Causal effect}

Treatment
$D_i=
\begin{cases}
  1 \text{, receiving treatment}\\
  0 \text{, not receiving treatment}
\end{cases}$

potential untreated outcome
$Y_{i}^{0}=Y_{i}(D=0)$

potential treated outcome
$Y_{i}^{1}=Y_{i}(D=1)$

realistic outcome
$Y_{i}=D_{i}Y_{i}^{1}-(1-D_{i})Y_{i}^{0}$

unit treatment effect
$\delta_{i}=Y_{i}^{1}-Y_{i}^{0}$

ATT
\begin{equation}
\begin{split}
\tau_{att}&=E(\delta_{i}|D_{i}=1)
\\&=E(Y^{1}_{i}-Y^{0}_{i}|D_{i}=1)
\\&=E(Y^{1}_{i}|D_{i}=1)-E(Y^{0}_{i}|D_{i}=1)
\end{split}
\end{equation}


ATU
\begin{equation}
\begin{split}
\tau_{atu}&=E(\delta_{i}|D_{i}=0)
\\&=E(Y^{1}_{i}-Y^{0}_{i}|D_{i}=0)
\\&=E(Y^{1}_{i}|D_{i}=0)-E(Y^{0}_{i}|D_{i}=0)
\end{split}
\end{equation}

ATE
\begin{equation}
\begin{split}
\tau_{ate}&=E(\delta_{i})
\\&=E(Y^{1}_{i}-Y^{0}_{i})
\\&=E[E(Y_{i}^{1}-Y_{i}^{0}|D_{i})]
\\&=E(Y_{i}^{1}-Y_{i}^{0}|D_{i}=1)\cdot Pr(D_{i}=1)+E(Y_{i}^{1}-Y_{i}^{0}|D_{i}=0)\cdot Pr(D_{i}=0)
\\&=\tau_{att}\cdot Pr(D_{i}=1)+\tau_{atu} \cdot Pr(D_{i}=0)
\end{split}
\end{equation}

\subsection{IV}

\textbf{IV conditions}
\begin{align}
  &corr(Z,\mu)=0
  \\&corr(Z,X)\neq 0
\end{align}

\textbf{2SLS}

For a 
$\begin{cases}
Y=\beta_0+\beta_1X+\mu\\
X=\pi_0+\pi_1 Z+\nu
\end{cases}$

step 1: regress $X$ on $Z$, eliminating the part of $X$ related to $\mu$

step 2: regress $Y$ on the estimated $\hat X$

step 3: resulting $\hat{\beta}=\frac{s_{YZ}}{s_{XZ}}$


\textbf{weak IV}

First stage least squares has F-value lower than 10. Or first stage regression is not significant.



\textbf{Identification}

$n=$number of IV and $k=$ number of endogenous variable

an identification problem can be denoted as 
$\begin{cases}
  n=k \text{ perfect identification}
  \\
  n>k \text{ over-identification}
  \\
  n<k \text{ unable to identify}
\end{cases}$

Sargent test

Hansen J test

C-statistics

\subsection{DID}

\begin{equation}
  y_{it}=\beta x_{it} + \gamma_1 D_i +\gamma_2 D_t + \mu_{it}
\end{equation}


\subsection{RDD}
\begin{equation}
  Y_i = \alpha + \beta D_i + f(X_i) + \mu_i 
\end{equation}

\begin{itemize}
\tightlist
  \item $D_i$ denotes if the treatment is received
  \item $X_i$ contains the treatment variable
  \item $f(\cdot)$ is to capture the continuity around the cut-off
\end{itemize}

sharp RD demands $W_i=1$ if $X_i\geqslant c$

fuzzy RD demands $\lim\limits_{x \rightarrow c^+} E(Y|X=x)\neq\lim\limits_{x \rightarrow c^-} E(Y|X=x)$

LATE of RDD $\tau=\lim\limits_{x \rightarrow c^+} E(Y|X=x)-\lim\limits_{x \rightarrow c^-} E(Y|X=x)$


\section{Time series data}

\textbf{auto regression}

AR(n) $\iff corr(X_t,X_{t-n})\neq 0$

\textbf{Stationary}

for $\{X_1,\dots,X_t,\dots\}$ that any sequence of N period has the same distribution

potential causes for being unstationary:
\begin{itemize}
\tightlist
  \item 
\end{itemize}


\textbf{unit root test}

study one time series data's stationary relationship. if unit root is tested true in the time series, the time series is not stationary.

common unit root tests are:
\begin{itemize}
\tightlist
  \item ADF test
  \item PP test
\end{itemize}

\textbf{cointegration test}

study multiple non-stationary time series' long-term stationary relationship.

\textbf{Granger test}

used for causal test in mutiple sationary time series


\section{Structural Equations}

\end{document}